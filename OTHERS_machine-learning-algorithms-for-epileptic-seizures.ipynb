{"cells":[{"metadata":{"_uuid":"52bfbfde1bba170d2347810de8f6889029c040f7"},"cell_type":"markdown","source":"# &#128204; Intro Of This Notebook\nThe aim of this study is to diagnose epileptic seizures by using different machine learning algorithms. For this purpose, the frequency components of the EEG are extracted by using the discrete wavelet transform (DWT) and parametric  methods based on autoregressive (AR) model. Both these two feature extraction methods are applied to the input of machine learning classification algorithms such as Artificial Neural Networks (ANN), Naive Bayesian, k-Nearest Neighbor (k-NN), Support Vector Machines (SVM), Logistic Regression,Principal Component Analysis. The results show that k-NN, ANN and SVM were the most efficient method according to test processing of both DWT and AR as feature extraction for recognition of epileptic seizures in EEG."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"6c2af32ca322bdd0686e244dd10fbb248409c7a1"},"cell_type":"code","source":"%%html\n<style>\n@import url('https://fonts.googleapis.com/css?family=Ewert|Roboto&effect=3d|ice|');\nbody {background-color: gainsboro;} \na {color: #37c9e1; font-family: 'Roboto';} \nh1 {color: #37c9e1; font-family: 'Orbitron'; text-shadow: 4px 4px 4px #aaa;} \nh2, h3 {color: slategray; font-family: 'Orbitron'; text-shadow: 4px 4px 4px #aaa;}\nh4 {color: #818286; font-family: 'Roboto';}\nspan {font-family:'Roboto'; color:black; text-shadow: 5px 5px 5px #aaa;}  \ndiv.output_area pre{font-family:'Roboto'; font-size:110%; color:lightblue;}      \n</style>","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# &#128225; Introduction Of Epileptic\nEpilepsy is a serious brain illness that is an endemic neurological disorder all over  the world. It is a clinical result that occurs with abnormal neurological electrical  discharging of brain. Epileptic seizures represent the most common positive signs and  symptoms of brain disturbance, and epilepsy is one of the most common primary  brain disorders . Vascular causes, traumatic causes, infections and brain abscesses,  brain tumors, nutritional deficiencies, pyridoxine deficiency, calcium metabolism  disorders are lead causes for epilepsy. For in diagnosing epilepsy, research is needed  for better understanding of mechanisms causing epileptic disorders. The evaluation  and treatment of neurophysiologic disorders are diagnosed with the  electroencephalogram [EEG]. EEG is crucial for accurate classification of different  forms of epilepsy ."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Theoretical Background\nThe aim of this study is to contribute to the diagnosis of epilepsy by taking advantage of the engineering. So, for diagnosing of epileptic seizures from EEG signals are transformed discrete wavelet and auto regressive models. After these transformations, extract data is applied input for Back-propagation, Naive Bayesian, k-Nearest Neighbor (k-NN), Support Vector Machines (SVM) ,ANN ,Logistic Regression and Principal Component Analysis algorithms."},{"metadata":{"_uuid":"f87ef7827d46d14e55f921b25c7d762175e941dc"},"cell_type":"markdown","source":"# EEG Data Recording\nEEG signals are separated into α, β, δ and θ spectral components and provide a  wide range of frequency components. EEG spectrum contains some characteristic  waveforms that fall primarily within four frequency bands as follows: δ(0.5-4 Hz),  θ(4-8 Hz), α(8-13 Hz), and β (13- 30 Hz) .\nEEG data set has acquired different age groups in this study. They are known  epileptic with uncontrolled seizures and are admitted to the neurology department of the Medical Faculty Hospital of Dicle University1. For this system LabView pro-  gramming language has been used  and the EEG data used in 400 people who re-  ceived 200 of them are epilepsy and with 200 of them are normal. Data set represents  of signals belong to several healthy and epileptic patients. The EEG signals that are  contained by PCI-MIO 16E DAQ card system that provides real time processing and  is a data bus of computer, signal processor and personal computer. Fig. 2 shows that  how to acquire EEG data from a patient [1]. EEG signals are to ensure the accuracy of  diagnosing disease that usually is taken 8-10 hours in the form of records. EEG sig-  nals are used in section and 23.6 seconds, 173 Hz sampling frequency is illustrated  with. International 10–20 electrode placement system according to the data collected,  12-bit analog-digital conversion after the samples are recorded subsequently. Data can  be passed through the filter 0.53–40 Hz band–pass, the EEG in the presence of clini-  cal interest for focusing range is provided. The EEG data used in our study were  downloaded from 24-h EEG recorded from both epileptic patients and normal sub-  jects. The following bipolar EEG channels were selected for analysis: F7-C3, F8-C4,  T5-O1 and T6-O2. In order to assess the performance of the classifier, we selected  500 EEG segments containing spike and wave complex, artifacts and background  normal EEG .\n![Imgur](https://i.imgur.com/6rQgYS9.png)"},{"metadata":{"trusted":true,"_uuid":"30e388d1b8ce7f8c9654828ef320fb39d953bab0"},"cell_type":"markdown","source":"# Discrete Wavelet Transform\nWavelet transform is more advantageous spectral analyze method than other  spectral analyze methods on non-stationary signals. Because the wavelet transform  method changes large low-frequency, high frequency that is narrow for the window  size. So, the entire frequency range can be achieved in the optimum time-frequency  resolution [22] Continuous and discrete wavelet transform is analyzed in the scale and  variation of parameters due to the continuous wavelet coefficients for each scale is  difficult and time consuming. For this reason, discrete wavelet transform is used more\nften than these non-stationary signals. Wavelet scale is divided into a number of  points for x[n] process as seen in Fig. 2 that is called multi resolution decomposition.  It is important that is selected appropriate wavelet decomposition level, the number of  detection and wavelet transform analysis of signals. Because of classification accura-  cy is dependent on type of wavelet, dominant frequency components of signals are  determined according to the number of decomposition levels.\nWavelet coefficients contain important information about EEG signal that provide  extraction of feature vector. Statistical-time frequency of EEG signals sequences are:\n\nThe average of the absolute value of coefficients in each sub-band.\nThe maximum absolute value of coefficients in each sub-band.\nThe mean force coefficients of each sub-band.\nStandard deviation of coefficients in each sub-band.\nThe average absolute value of the ratio of adjacent bands.\nDistribution of breakdown coefficients in each sub-band.\n\n1-3 sequence is signal characteristic; 4-6 sequence is that amount of frequency  change. This feature vector, of EEG signals that are used as inputs for multi-layer  neural network classification.\n![Imgur](https://i.imgur.com/Jzj3UAU.png)"},{"metadata":{"_uuid":"b9ad6f280e93376b5811f0cd1f25556150660b21"},"cell_type":"markdown","source":"# Importing the libraries"},{"metadata":{"trusted":true,"_uuid":"87d159a308dde25e6f89f58501552945f4c261cf","_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sn","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"710c7bc8e8f0ebf31b579ae609e2c8b364e4b731"},"cell_type":"markdown","source":"# Load the Dataset"},{"metadata":{"trusted":true,"_uuid":"ffb826c5e8da301b74c3a0161e7dcc079fec03ce","_kg_hide-input":true},"cell_type":"code","source":"ESR = pd.read_csv('../input/Epileptic Seizure Recognition.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f747e03448e282cdb4f5a611a8aa2191acc71c80"},"cell_type":"markdown","source":"# Read and Show Dataset\n* The original dataset from the reference consists of 5 different folders, each with 100 files, with each file representing a single subject/person. Each file is a recording of brain activity for 23.6 seconds.\n\n* The corresponding time-series is sampled into 4097 data points. Each data point is the value of the EEG recording at a different point in time. So we have total 500 individuals with each has 4097 data points for 23.5 seconds.\n\n* We divided and shuffled every 4097 data points into 23 chunks, each chunk contains 178 data points for 1 second, and each data point is the value of the EEG recording at a different point in time.\n\n* So now we have 23 x 500 = 11500 pieces of information(row), each information contains 178 data points for 1 second(column), the last column represents the label y {1,2,3,4,5}.\n\n* The response variable is y in column 179, the Explanatory variables X1, X2, ..., X178"},{"metadata":{"trusted":true,"_uuid":"98727073fd4a8acefbbe53251f25fe30986f1e2f","_kg_hide-input":true},"cell_type":"code","source":"ESR.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"17c90c1ff8add7ace30bd8d2cfb1fa5cb48c0e48"},"cell_type":"code","source":"cols = ESR.columns\ntgt = ESR.y\ntgt[tgt>1]=0\nax = sn.countplot(tgt,label=\"Count\")\nnon_seizure, seizure = tgt.value_counts()\nprint('The number of trials for the non-seizure class is:', non_seizure)\nprint('The number of trials for the seizure class is:', seizure)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b8358047321392e0f1a2fb20ca53c6668d8cdf3"},"cell_type":"markdown","source":"As we can see, there are 178 EEG features and 5 possible classes. The main goal of the dataset it's to be able to correctly identify epileptic seizures from EEG data, so a binary classification between classes of label 1 and the rest (2,3,4,5). In order to train our model, let's define our independent variables (X) and our dependent variable (y)."},{"metadata":{"trusted":true,"_uuid":"85c42387f7cbe1d864e6ecd78110d36543b49c94"},"cell_type":"markdown","source":"#  &#128223; Data Pre-processing\n\n## What is Data Pre-pocessing?\n![Imgur](https://i.imgur.com/HyPUqwF.png)\nData preprocessing is a data mining technique that involves transforming raw data into an understandable format. Real-world data is often incomplete, inconsistent, and/or lacking in certain behaviors or trends, and is likely to contain many errors. Data preprocessing is a proven method of resolving such issues. Data preprocessing prepares raw data for further processing.\n![Imgur](https://i.imgur.com/VuYZfho.jpg)"},{"metadata":{"trusted":true,"_uuid":"552d8f6aa05052620464a652d2a381c266a2fd98"},"cell_type":"markdown","source":"## &#128223; 1. Checking Missing Data"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"aad4b40423e626080316e9baf284b7ee75b64d08"},"cell_type":"code","source":"ESR.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc7b27a8bd5b7cea21b2dcd3a3c29676e18c1bb4"},"cell_type":"markdown","source":"###  &#128210; Note:\nThats great here is no missing value. So we can work very smoothly."},{"metadata":{"_uuid":"ab3a886cede2b110c7a6f5a28f42a15b478559e0"},"cell_type":"markdown","source":"# &#128203; Exploratory Data Analysis"},{"metadata":{"_uuid":"cdaf9a18dd4802a74fda29c78b594fdea389e2b1"},"cell_type":"markdown","source":"## &#128505; What is Exploratory data analysis?\n![Imgur](https://i.imgur.com/HyPUqwF.png)\nIn statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.\n\nYou can say that EDA is statisticians way of story telling where you explore data, find patterns and tells insights. Often you have some questions in hand you try to validate those questions by performing EDA. <b>I have one article on [EDA](https://hackernoon.com/overview-of-exploratory-data-analysis-with-python-6213e105b00b)"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"e88bcff9f978e728a07d29bd99eee000ec0e3bcc"},"cell_type":"code","source":"ESR.info()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"c59e74a086f2f41b14f793b599f0938cdab43704"},"cell_type":"code","source":"ESR.describe()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"38ab208871375f43e6afced6a807f23a85468a68"},"cell_type":"code","source":"X = ESR.iloc[:,1:179].values\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"1c4c72aceecdc35866aee2ec777a435abaf46fc3"},"cell_type":"code","source":"plt.subplot(511)\nplt.plot(X[1,:])\nplt.title('Classes')\nplt.ylabel('uV')\nplt.subplot(512)\nplt.plot(X[7,:])\nplt.subplot(513)\nplt.plot(X[12,:])\nplt.subplot(514)\nplt.plot(X[0,:])\nplt.subplot(515)\nplt.plot(X[2,:])\nplt.xlabel('Samples')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"32875b29c90672f90bdba34fcb94656dfce9fb70"},"cell_type":"code","source":"y = ESR.iloc[:,179].values\ny","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"182336e56e686533317d167bda1b5de60e3a3d14"},"cell_type":"markdown","source":"To make this a binary problem, let's turn the non-seizure classes 0 while maintaining the seizure as 1."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"67cdd58fc5bdc0fea9fff3a6c3eb030d17bb5fef"},"cell_type":"code","source":"y[y>1]=0\ny","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8f8bee91ce354f316e8ebb6d43f3a38d468c06e"},"cell_type":"markdown","source":"# &#128295; Building Machine Learning Models"},{"metadata":{"_uuid":"3935f14daec0df6ff948054196c77fdf6a8d9196"},"cell_type":"markdown","source":"##  Splitting the Dataset into the Training set and Test set\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"98ca8b335764e01df2b8aaa10183a4e577854d30"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"215e46fbe236162ccd3f551bdd4f58c4e13f30fd"},"cell_type":"markdown","source":"## Feature Scaling"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"7b5ac7857321848b984fbe95957e7078066daf75"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1de8aa4a8362c101e7bbd3a3376257dd7ed03a9f"},"cell_type":"markdown","source":"# 1. Logistic Regression\n![Imgur](https://i.imgur.com/HyPUqwF.png)\nLogistic regression, or logit regression, or logit model is a regression model where the dependent variable (DV) is categorical. This article covers the case of a binary dependent variable—that is, where it can take only two values, \"0\" and \"1\", which represent outcomes such as pass/fail, win/lose, alive/dead or healthy/sick. Cases where the dependent variable has more than two outcome categories may be analysed in multinomial logistic regression, or, if the multiple categories are ordered, in ordinal logistic regression."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"bb41b354adaf04dc6ce4a4a3204fe5bd8df629bc"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\ny_pred_log_reg = clf.predict(X_test)\nacc_log_reg = round(clf.score(X_train, y_train) * 100, 2)\nprint (str(acc_log_reg) + ' %')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fc90f5ec9c604aaafa2de62e676556cf206292b"},"cell_type":"markdown","source":"# 2.Support Vector Machine (SVM)\n![Imgur](https://i.imgur.com/HyPUqwF.png)\nSupport Vector Machine (SVM) model is a Supervised Learning model used for classification and regression analysis. It is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\n\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support vector machines, a data point is viewed as a  p -dimensional vector (a list of  p  numbers), and we want to know whether we can separate such points with a  (p−1) -dimensional hyperplane.\n\nWhen data are not labeled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The clustering algorithm which provides an improvement to the support vector machines is called support vector clustering and is often used in industrial applications either when data are not labeled or when only some data are labeled as a preprocessing for a classification pass.\n\nIn the below code, SVC stands for Support Vector Classification."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"099f1945de2146e22b4cb0abbc5e128bdab827d3"},"cell_type":"code","source":"from sklearn.svm import SVC\nclf = SVC()\nclf.fit(X_train, y_train)\ny_pred_svc = clf.predict(X_test)\nacc_svc = round(clf.score(X_train, y_train) * 100, 2)\nprint (str(acc_svc) + '%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f51852f983d909d7ed641c720e8e45064ebe3041"},"cell_type":"markdown","source":"# Linear SVM"},{"metadata":{"trusted":true,"_uuid":"171a3d7c5beba58348c4a2c878ebf7dfe4f703c8"},"cell_type":"code","source":"from sklearn.svm import SVC, LinearSVC\nclf = LinearSVC()\nclf.fit(X_train, y_train)\ny_pred_linear_svc = clf.predict(X_test)\nacc_linear_svc = round(clf.score(X_train, y_train) * 100, 2)\nprint (str(acc_linear_svc) + '%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7e214c66173633d35e15e637043b2aa4b782fe5"},"cell_type":"markdown","source":"# 3.*k*-Nearest Neighbors\n![Imgur](https://i.imgur.com/HyPUqwF.png)\n*k* -nearest neighbors algorithm (k-NN) is one of the simplest machine learning algorithms and is used for classification and regression. In both cases, the input consists of the  k  closest training examples in the feature space. The output depends on whether  k -NN is used for classification or regression:\n\n* In  k -NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its  k  nearest neighbors ( k  is a positive integer, typically small). If  k=1 , then the object is simply assigned to the class of that single nearest neighbor.\n\n* In  k -NN regression, the output is the property value for the object. This value is the average of the values of its  k nearest neighbors."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"93e7e59da21b3d12737061c6687cb628d0960dfb"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier()\nclf.fit(X_train, y_train)\ny_pred_knn = clf.predict(X_test)\nacc_knn = round(clf.score(X_train, y_train) * 100, 2)\nprint (str(acc_knn)+'%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f64a172f160a43d9ebd5f5ff988de9ba7084bb1"},"cell_type":"markdown","source":"# 4. Gaussian Naive Bayes\n![Imgur](https://i.imgur.com/HyPUqwF.png)\nNaive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\n\nBayes' theorem (alternatively Bayes' law or Bayes' rule) describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if cancer is related to age, then, using Bayes' theorem, a person's age can be used to more accurately assess the probability that they have cancer, compared to the assessment of the probability of cancer made without knowledge of the person's age.\n\nNaive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. It is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter. A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"6090e8aa7daae6d6aef32152a077d867beef9d1b"},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(X_train, y_train)\ny_pred_gnb = clf.predict(X_test)\nacc_gnb = round(clf.score(X_train, y_train) * 100, 2)\nprint (str(acc_gnb) + '%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"724afa104f8fc061f379adb0ba408730e024abeb"},"cell_type":"markdown","source":"# 5. Artificial Neural Networks(ANN)\n![Imgur](https://i.imgur.com/HyPUqwF.png)\nComputers are great at solving algorithmic and math problems, but often the world can't easily be defined with a mathematical algorithm. Facial recognition and language processing are a couple of examples of problems that can't easily be quantified into an algorithm, however these tasks are trivial to humans. The key to Artificial Neural Networks is that their design enables them to process information in a similar way to our own biological brains, by drawing inspiration from how our own nervous system functions. This makes them useful tools for solving problems like facial recognition, which our biological brains can do easily."},{"metadata":{"_uuid":"a62b3d36e974853cec28f77b5a195a24bf04f5f4"},"cell_type":"markdown","source":"## Modeling Artificial Neurons\nArtificial neuron models are at their core simplified models based on biological neurons. This allows them to capture the essence of how a biological neuron functions. We usually refer to these artificial neurons as 'perceptrons'. So now lets take a look at what a perceptron looks like.\n![Imgur](https://i.imgur.com/uvMRQ6R.jpg)\nAs shown in the diagram above a typical perceptron will have many inputs and these inputs are all individually weighted. The perceptron weights can either amplify or deamplify the original input signal. For example, if the input is 1 and the input's weight is 0.2 the input will be decreased to 0.2. These weighted signals are then added together and passed into the activation function. The activation function is used to convert the input into a more useful output. There are many different types of activation function but one of the simplest would be step function. A step function will typically output a 1 if the input is higher than a certain threshold, otherwise it's output will be 0."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"ef70904940d3a29a09db064e4264729878a88798"},"cell_type":"code","source":"#Importing keras libraries and packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1cb8e842cc4865e02f3cd620ed2d1ce36529876"},"cell_type":"markdown","source":"## Implementing Artificial Neural Networks\nSo now you're probably wondering what an artificial neural network looks like and how it uses these artificial neurons to process information. In this tutorial we're going to be looking at feedforward networks and how their design links our perceptron together creating a functioning artificial neural network. Before we begin lets take a look at what a basic feedforward network looks like:\n![Imgur](https://i.imgur.com/la4Rwn6.jpg)\nEach input from the input layer is fed up to each node in the hidden layer, and from there to each node on the output layer. We should note that there can be any number of nodes per layer and there are usually multiple hidden layers to pass through before ultimately reaching the output layer. Choosing the right number of nodes and layers is important later on when optimising the neural network to work well a given problem. As you can probably tell from the diagram, it's called a feedforward network because of how the signals are passed through the layers of the neural network in a single direction. These aren't the only type of neural network though. There are also feedback networks where its architecture allows signals to travel in both directions."},{"metadata":{"_uuid":"17903f4e7acd699e10feddda3f2ede2f122ba0d5"},"cell_type":"markdown","source":"### Initializing the ANN"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d5227ec581d362265d05d1ed25c12ac8f443464d"},"cell_type":"code","source":"classifier = Sequential()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a9c01703ffd3ac194233a5e862225f3e010a80b"},"cell_type":"markdown","source":"### Adding input layer and first hidden layer"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"5cc825b56c5217cb6a39b696bc5f6e2552814d69"},"cell_type":"code","source":"classifier.add(Dense(output_dim = 80, init = 'uniform', activation = 'relu', input_dim = 178))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a50585c8466e7c04739e18ae53cca8888282bce7"},"cell_type":"markdown","source":"### Adding second hidden layer"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"4d28df06df33846c0004b4461e7f67ef1bf78c5c"},"cell_type":"code","source":"classifier.add(Dense(output_dim = 80, init = 'uniform', activation = 'relu'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9b816ba42105e4d2d9f8ae465c59bc92eddd0b7"},"cell_type":"markdown","source":"### Adding the output layer"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"b783f126f5845942a7fc9c46717cba4202527647"},"cell_type":"code","source":"classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34352f3b099615aa9161cd4426ca522a91c4cf77"},"cell_type":"markdown","source":"### Compiling the ANN"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"0b1f53b91b238b641147d384922834e0b562d6df"},"cell_type":"code","source":"classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n#Fitting the ANN to the training set\nclassifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d6d20a37e919957eca21406a579f230c69dddf4d"},"cell_type":"code","source":"# Predicting the Test set results\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)\nacc_ANN = round(clf.score(X_train, y_train) * 100, 2)\nprint (str(acc_ANN) + '%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ca8da32b15bea0d9a05d29b2b870e7b5e3d6f8c"},"cell_type":"markdown","source":"# Principal Component Analysis (PCA)\n![Imgur](https://i.imgur.com/HyPUqwF.png)\nPrincipal\tComponent\tAnalysis\t(PCA)is a dimension-reduction tool that can be used to reduce a large set of variables to a small set that still contains most of the information in the large set.\n\nHow does PCA work -\n\n1. Calculate the covariance matrix X of data points.\n2. Calculate eigen vectors and corresponding eigen values.\n3. Sort the eigen vectors according to their eigen values in decreasing order.\n4. Choose first k eigen vectors and that will be the new k dimensions.\n5. Transform the original n dimensional data points into k dimensions."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"61a34e727af945a625193d25df16c3941e451cf7"},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA()\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\nacc_PCA = round(pca.score(X_train, y_train) )\nprint (str(acc_PCA) + '%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"572251b49c99a3c6e27d0945cb9fcc53e7a29662"},"cell_type":"markdown","source":"# Comparing Models"},{"metadata":{"trusted":true,"_uuid":"646bccff6126c0b579594a3217c0fa045473abfd"},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Support Vector Machines','ANN', \n              'KNN', 'Naive Bayes', 'Principal Component Analysis'],\n    \n    'Score': [acc_log_reg, acc_svc,acc_ANN , \n              acc_knn, acc_gnb,acc_PCA ]\n    })\n\nmodels.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a53f723605eb5f1852bb9b094ebba1d6fe035f3c"},"cell_type":"markdown","source":"# Conclusion\nThe aim of this study is to detect epileptic seizure using two different feature ex-traction methods and comparison performance of various machine learning algorithms."},{"metadata":{"_uuid":"391bfcd4dc814d73082cfeb5a911a791a85c0a9a"},"cell_type":"markdown","source":"# References\n1. [Epileptic Seizure Recognition Research Paper](http://iwbbio.ugr.es/2014/papers/IWBBIO_2014_paper_1.pdf)\n\n2. [Epileptics Seziure Recognition Github's HelpNotebook](https://github.com/erayon/seizures/blob/master/seizures_.ipynb)\n\n3. [Epileptic Seizure Recognition Video](https://www.youtube.com/watch?v=sq0AhHtknBU)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}