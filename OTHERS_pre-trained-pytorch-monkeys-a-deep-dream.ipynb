{"cells":[{"metadata":{"_uuid":"e57a482c96f2678272e658bc7fbd6a1574f0f7ec"},"cell_type":"markdown","source":"**Pre-trained PyTorch Monkeys: A Deep Dream\n**\n\nThe deep dream enhances features in the input image that are recognized by the pre-trained network.  First we will look at an image of a bald-headed uakari monkey and then we will enhance the features from that image that are recognized by the VGG16 and DenseNet121 pre-trained models.  Finally, we will use an image of a banana to guide our dream in a new direction.  \n\nThis work is an adaptation of the following kernel: https://www.kaggle.com/carloalbertobarbano/convolutional-network-visualizations-deep-dream"},{"metadata":{"_uuid":"04ea043d76780d5dd39f8d6673b9d6b850b8bdff","_cell_guid":"d8437be7-1af2-4056-9c57-1c751ec71b79","colab_type":"code","id":"6uznQ7thrODO","colab":{"autoexec":{"startup":false,"wait_interval":0}},"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# https://www.kaggle.com/carloalbertobarbano/convolutional-network-visualizations-deep-dream\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom torch.optim import SGD\nfrom torchvision import models, transforms\nimport PIL\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\nfrom IPython.display import HTML\nimport scipy.ndimage as ndimage\n%matplotlib inline\nimport scipy.ndimage as nd\nimport PIL.Image\nfrom IPython.display import clear_output, Image, display\nfrom io import BytesIO\n\ndef showarray(a, fmt='jpeg'):\n    a = np.uint8(np.clip(a, 0, 255))\n    f = BytesIO()\n    PIL.Image.fromarray(a).save(f, fmt)\n    display(Image(data=f.getvalue()))\n    \ndef showtensor(a):\n    mean = np.array([0.485, 0.456, 0.406]).reshape([1, 1, 3])\n    std = np.array([0.229, 0.224, 0.225]).reshape([1, 1, 3])\n    inp = a[0, :, :, :]\n    inp = inp.transpose(1, 2, 0)\n    inp = std * inp + mean\n    inp *= 255\n    showarray(inp)\n    clear_output(wait=True)\n\ndef plot_images(im, titles=None):\n    plt.figure(figsize=(30, 20))\n    \n    for i in range(len(im)):\n        plt.subplot(10 / 5 + 1, 5, i + 1)\n        plt.axis('off')\n        if titles is not None:\n            plt.title(titles[i])\n        plt.imshow(im[i])\n        \n    plt.pause(0.001)\n    \nnormalise = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nnormalise_resize = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ndef init_image(size=(400, 400, 3)):\n    img = PIL.Image.fromarray(np.uint8(np.full(size, 150)))\n    img = PIL.Image.fromarray(np.uint8(np.random.uniform(150, 180, size)))\n    img_tensor = normalise(img).unsqueeze(0)\n    img_np = img_tensor.numpy()\n    return img, img_tensor, img_np\n\ndef load_image(path, resize=False, size=None):\n    img = PIL.Image.open(path)\n    \n    if size is not None:\n        img.thumbnail(size, PIL.Image.ANTIALIAS)\n        \n    if resize:\n        img_tensor = normalise_resize(img).unsqueeze(0)\n    else:\n        img_tensor = normalise(img).unsqueeze(0)\n    img_np = img_tensor.numpy()\n    return img, img_tensor, img_np\n\ndef tensor_to_img(t):\n    a = t.numpy()\n    mean = np.array([0.485, 0.456, 0.406]).reshape([1, 1, 3])\n    std = np.array([0.229, 0.224, 0.225]).reshape([1, 1, 3])\n    inp = a[0, :, :, :]\n    inp = inp.transpose(1, 2, 0)\n    inp = std * inp + mean\n    inp *= 255\n    inp = np.uint8(np.clip(inp, 0, 255))\n    return PIL.Image.fromarray(inp)\n\ndef image_to_variable(image, requires_grad=False, cuda=False):\n    if cuda:\n        image = Variable(image.cuda(), requires_grad=requires_grad)\n    else:\n        image = Variable(image, requires_grad=requires_grad)\n    return image\n\ndef octaver_fn(model, base_img, step_fn, octave_n=6, octave_scale=1.4, iter_n=10, **step_args):\n    octaves = [base_img]\n    \n    for i in range(octave_n - 1):\n        octaves.append(nd.zoom(octaves[-1], (1, 1, 1.0 / octave_scale, 1.0 / octave_scale), order=1))\n\n    detail = np.zeros_like(octaves[-1])\n    for octave, octave_base in enumerate(octaves[::-1]):\n        h, w = octave_base.shape[-2:]\n        \n        if octave > 0:\n            h1, w1 = detail.shape[-2:]\n            detail = nd.zoom(detail, (1, 1, 1.0 * h / h1, 1.0 * w / w1), order=1)\n        \n        src = octave_base + detail\n        \n        for i in range(iter_n):\n            src = step_fn(model, src, **step_args)\n\n        detail = src.numpy() - octave_base\n\n    return src\n\ndef filter_step(model, img, layer_index, filter_index, step_size=5, display=True, use_L2=False):\n    global use_gpu\n    \n    mean = np.array([0.485, 0.456, 0.406]).reshape([3, 1, 1])\n    std = np.array([0.229, 0.224, 0.225]).reshape([3, 1, 1])\n    \n    model.zero_grad()\n    \n    img_var = image_to_variable(torch.Tensor(img), requires_grad=True, cuda=use_gpu)\n    optimizer = SGD([img_var], lr=step_size, weight_decay=1e-4)\n    \n    x = img_var\n    for index, layer in enumerate(model.features):\n        x = layer(x)\n        if index == layer_index:\n            break\n\n    output = x[0, filter_index]\n    loss = output.norm() #torch.mean(output)\n    loss.backward()\n    \n    if use_L2:\n        #L2 normalization on gradients\n        mean_square = torch.Tensor([torch.mean(img_var.grad.data ** 2) + 1e-5])\n        if use_gpu:\n            mean_square = mean_square.cuda()\n        img_var.grad.data /= torch.sqrt(mean_square)\n        img_var.data.add_(img_var.grad.data * step_size)\n    else:\n        optimizer.step()\n    \n    result = img_var.data.cpu().numpy()\n    result[0, :, :, :] = np.clip(result[0, :, :, :], -mean / std, (1 - mean) / std)\n    \n    if display:\n        showtensor(result)\n    \n    return torch.Tensor(result)\n\ndef visualize_filter(model, base_img, layer_index, filter_index, \n                     octave_n=6, octave_scale=1.4, iter_n=10, \n                     step_size=5, display=True, use_L2=False):\n    \n    return octaver_fn(\n                model, base_img, step_fn=filter_step, \n                octave_n=octave_n, octave_scale=octave_scale, \n                iter_n=iter_n, layer_index=layer_index, \n                filter_index=filter_index, step_size=step_size, \n                display=display, use_L2=use_L2\n            )\n\ndef show_layer(layer_num, filter_start=10, filter_end=20, step_size=7, use_L2=False):\n    filters = []\n    titles = []\n    \n    _, _, img_np = init_image(size=(600, 600, 3))\n    for i in range(filter_start, filter_end):\n        title = \"Layer {} Filter {}\".format(layer_num , i)\n        print(title)\n        filter = visualize_filter(model, img_np, layer_num, filter_index=i, octave_n=2, iter_n=20, step_size=step_size, display=True, use_L2=use_L2)\n        filter_img = tensor_to_img(filter)\n        filter_img.save(title + \".jpg\")\n        filters.append(tensor_to_img(filter))\n        titles.append(title)\n        \n    \n    plot_images(filters, titles)\n    return filters, titles\n\ndef objective(dst, guide_features):\n    if guide_features is None:\n        return dst.data\n    else:\n        x = dst.data[0].cpu().numpy()\n        y = guide_features.data[0].cpu().numpy()\n        ch, w, h = x.shape\n        x = x.reshape(ch, -1)\n        y = y.reshape(ch, -1)\n        A = x.T.dot(y)\n        diff = y[:, A.argmax(1)]\n        if use_gpu:\n            diff = torch.Tensor(np.array([diff.reshape(ch, w, h)])).cuda()\n        else: \n            diff = torch.Tensor(np.array([diff.reshape(ch, w, h)])) \n        return diff\n\ndef make_step(model, img, objective=objective, control=None, step_size=1.5, end=28, jitter=32):\n    global use_gpu\n    \n    mean = np.array([0.485, 0.456, 0.406]).reshape([3, 1, 1])\n    std = np.array([0.229, 0.224, 0.225]).reshape([3, 1, 1])\n    \n    ox, oy = np.random.randint(-jitter, jitter+1, 2)\n    \n    img = np.roll(np.roll(img, ox, -1), oy, -2)\n    tensor = torch.Tensor(img) \n    \n    img_var = image_to_variable(tensor, requires_grad=True, cuda=use_gpu)\n    model.zero_grad()\n      \n    x = img_var\n    for index, layer in enumerate(model.features.children()):\n        x = layer(x)\n        if index == end:\n            break\n    \n    delta = objective(x, control)\n    x.backward(delta)\n    \n    #L2 Regularization on gradients\n    mean_square = torch.Tensor([torch.mean(img_var.grad.data ** 2)])\n    if use_gpu:\n        mean_square = mean_square.cuda()\n    img_var.grad.data /= torch.sqrt(mean_square)\n    img_var.data.add_(img_var.grad.data * step_size)\n    \n    result = img_var.data.cpu().numpy()\n    result = np.roll(np.roll(result, -ox, -1), -oy, -2)\n    result[0, :, :, :] = np.clip(result[0, :, :, :], -mean / std, (1 - mean) / std)\n    showtensor(result)\n    \n    return torch.Tensor(result)\n                                                             \ndef deepdream(model, base_img, octave_n=6, octave_scale=1.4, \n              iter_n=10, end=28, control=None, objective=objective, \n              step_size=1.5, jitter=32):\n    \n    return octaver_fn(\n              model, base_img, step_fn=make_step, \n              octave_n=octave_n, octave_scale=octave_scale, \n              iter_n=iter_n, end=end, control=control,\n              objective=objective, step_size=step_size, jitter=jitter\n           )","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"c92171f10eb28b9a51a7a5ba70858ab5d3f9b9ff"},"cell_type":"markdown","source":"First we will use features from the DenseNet121 pre-trained model"},{"metadata":{"trusted":true,"_uuid":"892421f03c631affbcf44b3c1b8eda01c974e78c","_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"inputImage = '../input/10-monkey-species/training/training/n2/n2133.jpg'\nguideImage = '../input/fruits/fruits-360_dataset/fruits-360/Training/Banana/254_100.jpg'\nmodel = models.densenet121() # add pretrained=True if not on Kaggle\nmodel.load_state_dict(torch.load(\"../input/densenet121/densenet121.pth\"))\n\nuse_gpu = False\nif torch.cuda.is_available():\n    use_gpu = True\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nif use_gpu:\n    #print(\"Using CUDA\")\n    model.cuda()","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"863fb94188c95aef2aa6f3bbb786f697eab8ed43","colab":{"autoexec":{"startup":false,"wait_interval":0},"height":747,"base_uri":"https://localhost:8080/"},"_cell_guid":"149ca97f-00b7-43c3-a8e0-871f8efe77d3","executionInfo":{"elapsed":4410,"status":"ok","timestamp":1526053309796,"user":{"photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg","userId":"107843268563316278814","displayName":"Carlo Alberto"},"user_tz":-120},"colab_type":"code","outputId":"bd9f48b8-c66a-43fc-a987-f7d31920af0b","id":"XZB9hS2zyVLt","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"input_img, input_tensor, input_np = load_image(inputImage, size=[1024, 1024])\n#print(input_img.size)\ninput_img","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4247b2ceb5987ee6532a8681b38a28708125102","colab":{"autoexec":{"startup":false,"wait_interval":0},"height":730,"base_uri":"https://localhost:8080/"},"_cell_guid":"d045dbee-8cba-47f6-9ff6-c8e40a3d3667","executionInfo":{"elapsed":11,"status":"ok","timestamp":1526053406741,"user":{"photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg","userId":"107843268563316278814","displayName":"Carlo Alberto"},"user_tz":-120},"colab_type":"code","outputId":"4b8a8981-85cd-4632-9224-6456ea2c2854","id":"pwPcM7iIyrbV","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dream = deepdream(model, input_np, end=7, step_size=0.06, octave_n=6)\ndream = tensor_to_img(dream)\ndream.save('densenet121dream.jpg')\ndream","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"588da3147d76131c49213113be27ee6b8fc44b3c","colab":{"autoexec":{"startup":false,"wait_interval":0},"height":260,"base_uri":"https://localhost:8080/"},"_cell_guid":"7fc0ddc6-9caa-40da-a6ac-382046d27ebd","executionInfo":{"elapsed":1017,"status":"ok","timestamp":1526054100646,"user":{"photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg","userId":"107843268563316278814","displayName":"Carlo Alberto"},"user_tz":-120},"colab_type":"code","outputId":"7a169f5b-1d77-4de3-b837-c8e4e29abfa0","id":"S97NHU7sY8Yf","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"guide_img, guide_img_tensor, guide_img_np = load_image(guideImage, resize=True)\nplt.imshow(guide_img)\nguide_features = image_to_variable(guide_img_tensor, cuda=use_gpu)\nend=28\nfor index, layer in enumerate(model.features.children()):\n    guide_features = layer(guide_features)\n    if index == end:\n        break ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2cabeef0eb4ef98b9fa1a3c982833065429bb2d","colab":{"autoexec":{"startup":false,"wait_interval":0},"height":730,"base_uri":"https://localhost:8080/"},"_cell_guid":"2fe6d591-8926-4f48-b4a9-d2ead3081649","executionInfo":{"elapsed":17,"status":"ok","timestamp":1526054232473,"user":{"photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg","userId":"107843268563316278814","displayName":"Carlo Alberto"},"user_tz":-120},"colab_type":"code","outputId":"4ebbbf4a-4bef-45a5-ed55-14aa40435172","id":"tSJ3RNxaZlkH","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dream = deepdream(model, input_np, end=7, step_size=0.06, octave_n=10, control=guide_features)\ndream = tensor_to_img(dream)\ndream.save('densenet121_guided_dream.jpg')\ndream","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cd2395469b81ece0c89ca66a836016cdebef992"},"cell_type":"markdown","source":"Now we will use features from the VGG-16 pre-trained model"},{"metadata":{"trusted":true,"_uuid":"52d6f61bd9e764b1c1a5868870d8cbf1397b9c43","_kg_hide-input":true},"cell_type":"code","source":"input_img, input_tensor, input_np = load_image(inputImage, size=[1024, 1024])\n#print(input_img.size)\ninput_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"604dc88c4597b3bb389dbf8dbefe84325ccc1fed","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"inputImage = '../input/10-monkey-species/training/training/n2/n2133.jpg'\nguideImage = '../input/fruits/fruits-360_dataset/fruits-360/Training/Banana/254_100.jpg'\nmodel = models.vgg16() # add pretrained=True if not on Kaggle\nmodel.load_state_dict(torch.load(\"../input/vgg16/vgg16.pth\"))\nuse_gpu = False\nif torch.cuda.is_available():\n    use_gpu = True\nfor param in model.parameters():\n    param.requires_grad = False\nif use_gpu:\n    #print(\"Using CUDA\")\n    model.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c28658efc838f1f85c14907f5e0b3c0534a4390b","_kg_hide-input":true},"cell_type":"code","source":"dream = deepdream(model, input_np, end=28, step_size=0.06, octave_n=6)\ndream = tensor_to_img(dream)\ndream.save('vgg16_dream.jpg')\ndream","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49af2ff8d5445e0b8ecb94b1e38214d7af2c7de2","_kg_hide-input":true},"cell_type":"code","source":"guide_img, guide_img_tensor, guide_img_np = load_image(guideImage, resize=True)\nplt.imshow(guide_img)\nguide_features = image_to_variable(guide_img_tensor, cuda=use_gpu)\nend=28\nfor index, layer in enumerate(model.features.children()):\n    guide_features = layer(guide_features)\n    if index == end:\n        break ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afc16322910ce416e575f7a594d5497e0a140eb2","_kg_hide-input":true},"cell_type":"code","source":"dream = deepdream(model, input_np, end=30, step_size=0.08, octave_n=6, control=guide_features)\ndream = tensor_to_img(dream)\ndream.save('vgg16_guided_dream.jpg')\ndream","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46abe2a1b15edc5e7986f641326373b328ea667b"},"cell_type":"markdown","source":"One last monkey:"},{"metadata":{"trusted":true,"_uuid":"93577f6978f07aeff7db33d7531d0a96cf9bc381","collapsed":true},"cell_type":"code","source":"dream = deepdream(model, input_np, end=28, step_size=0.1, octave_n=6)\ndream = tensor_to_img(dream)\ndream.save('vgg16_dream.jpg')\ndream","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d1e9a3f871b394db1bf883a663fc66c71f7182a"},"cell_type":"markdown","source":"First we looked at an image of a bald-headed uakari monkey and then we enhanced the features from that image that were recognized by the VGG16 and DenseNet121 pre-trained models. \n\nFor more detail about convolutional networks and deep dreams, see: https://www.kaggle.com/carloalbertobarbano/convolutional-network-visualizations-deep-dream"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"default_view":{},"provenance":[],"name":"CNN Visualization.ipynb","collapsed_sections":[],"version":"0.3.2","views":{}}},"nbformat":4,"nbformat_minor":1}